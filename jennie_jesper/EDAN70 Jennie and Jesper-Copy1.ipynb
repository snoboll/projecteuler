{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-03-31\n",
    "Aim: fix comm_use_subset_100, learn structure of dataset and annotation, learn jupyter, generate metafile\n",
    "\n",
    "\n",
    "Getting subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil\n",
    "\n",
    "comm_use_subset_100 = random.sample(os.listdir(\"/Users/jesperlaurell/EDAN70/comm_use_subset\"), 100)\n",
    "\n",
    "for filename in comm_use_subset_100:\n",
    "    shutil.copy(\"/Users/jesperlaurell/EDAN70/comm_use_subset/\" + filename, '/Users/jesperlaurell/EDAN70/comm_use_subset_100')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After generating the subset by running the above code once a new metafile was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os\n",
    "\n",
    "parentpath = str(pathlib.Path(__file__).parent.absolute())\n",
    "comm_use_subset_100 = parentpath + \"/comm_use_subset_100\"\n",
    "\n",
    "filenames = []\n",
    "for f in os.listdir(comm_use_subset_100):\n",
    "    print(f[:-5])\n",
    "    filenames.append(f[:-5])\n",
    "\n",
    "out = open(\"meta_subset_100.csv\",\"w+\")\n",
    "\n",
    "for line in open(\"metadata.csv\", \"r\"):\n",
    "    for f in filenames:\n",
    "        if(f in line):\n",
    "            print(f)\n",
    "            out.write(line)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Conclusion/Next steps: The tagger is not yet complete since we need to ensure we have the correct format"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-04-03\n",
    "\n",
    "Aim: work on tagger\n",
    "Conclusion/Next steps: finish tagger, needs to solve denotation part"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-04-07\n",
    "\n",
    "Aim: Finish tagger\n",
    "Conclusions: Tried to find the \"best match\" with help of methods get_longest_match an is_overlapping, did not work as wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_match(subsection, d):\n",
    "     longestmatch = \"\"\n",
    "     longestbegin = -1\n",
    "     longestend = -1\n",
    "     for phrase in d:\n",
    "          begin = subsection.find(phrase)\n",
    "          end = begin + len(phrase)\n",
    "          if begin > 0:\n",
    "               if is_overlapping(begin, end, longestbegin, longestend):\n",
    "                    if len(longestmatch) < len(phrase):\n",
    "                         longestmatch = phrase\n",
    "                         longestbegin = begin\n",
    "                         longestend = begin + len(longestmatch)\n",
    "               elif longestmatch == \"\":\n",
    "                    longestmatch = phrase\n",
    "                    longestbegin = begin\n",
    "                    longestend = begin + len(longestmatch)\n",
    "\n",
    "     if not longestmatch == \"\":\n",
    "          return longestbegin, longestmatch, longestend\n",
    "     else:\n",
    "          return None, None, None\n",
    "\n",
    "def is_overlapping(b1, e1, b2, e2):\n",
    "     return not (e2<b1 or e1<b2)\n",
    "     \n",
    "\n",
    "def tag_article(article_path):\n",
    "     article = read_article(article_path)\n",
    "     section_nr = 0\n",
    "     denotated_sections = []\n",
    "\n",
    "     for section in article:\n",
    "          for subsection in section:\n",
    "               subsection = subsection.lower()\n",
    "               denotations = []\n",
    "               longestmatch = \"\"\n",
    "               for id in dicts.keys():\n",
    "                    longestbegin, longestmatch, longestend = get_longest_match(subsection, dicts[id])\n",
    "                    info = {\"id\": id, \"span\":{\"begin\":longestbegin, \"end\":longestend}, \"obj\":\"?\"}\n",
    "                    if not longestmatch == None:\n",
    "                         denotations.append(info)\n",
    "                         print(\"found\", longestmatch)\n",
    "                              \n",
    "               denotated_sections.append(denotations)\n",
    "     return denotated_sections\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next step: Make the code for the tagger work properly, finding \"best match\" and generate correct JSON files."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-04-16\n",
    "Aim: Work on tagger\n",
    "Conclusions: Use regEx to find the longest/\"best\" match from the dictionary instead of the methods built 2020-04-07. Succeded to find best match with updated tag_article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next step: Fix correct JSON files. Test format at pubannotation.org, test run code with silverstandard dataset and evaluate the performance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-04-22\n",
    "Aim: Finish tagger, Json files needs to be generated correctly. Test the format at pubannotations.org. Test\n",
    "\n",
    "Conclusions: Tagger works as wanted and when testing at pubannotion.org we got a valid result. Still a bit unsure about what obj in denotations is supposed to be."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-04-28\n",
    "\n",
    "Had meeting with other dict group discussing approach for evaluation and current state."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-05-01\n",
    "\n",
    "Worked with the evaluation, setting up structure for the script. Also fixed some bugs in the tagger."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "2020-05-04\n",
    "\n",
    "Working with evaluation. Good progress but some critical issues:\n",
    "1. one of the files in the given \"updated_gold_standard.json\" is missing a cord_uid\n",
    "2. none of the files in \"updated_gold_standard.json\" has a div_id\n",
    "3. one of the files in \"updated_gold_standard.json\" has an associated metafile with no paperid\n",
    "\n",
    "These three issues make it difficult to compare the files and could give a much lower performance than expected.\n",
    "\n",
    "First version of the evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pathlib, json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def json_to_dict(json_path):\n",
    "    json = open(json_path,\"r\")\n",
    "    for line in json:\n",
    "        dict.append = json.loads(line)\n",
    "    return dict\n",
    "\n",
    "\n",
    "def find_real_entities_retrived(dict_gold, denotation):\n",
    "    if dict_gold[\"text\"] == denotation[\"text\"]\n",
    "        real_entities_retrived = 0\n",
    "        for denot_gold in dict_gold[\"denotations\"]:\n",
    "            span_gold = denot_gold[\"span\"]\n",
    "            for denot in denotation[\"denotations\"]\n",
    "                if span_gold == denot[\"span\"]:\n",
    "                    real_entities_retrived += 1\n",
    "\n",
    "    return real_entities_retrived\n",
    "\n",
    "\n",
    "def find_real_entities(dict_gold):\n",
    "    real_entities = 0\n",
    "    for denot_gold in dict_gold[\"denotations\"]:\n",
    "        real_entities += 1\n",
    "return real_entities\n",
    "\n",
    "\n",
    "def find_entities_retrived(denot)\n",
    "    entities_retrived = 0\n",
    "    for denot_line in denot[\"denotations\"]:\n",
    "        entities_retrived += 1\n",
    "return entities_retrived\n",
    "\n",
    "\n",
    "def Recall(real_entities, real_entities_retrived):\n",
    "    recall = real_entities_retrived/real_entities\n",
    "    return recall\n",
    "\n",
    "\n",
    "def Precision(real_entities_retrived,entities_retrived):\n",
    "    precision = real_entities_retrived/entities_retrived\n",
    "    return precision\n",
    "\n",
    "\n",
    "def main():\n",
    "    gold_std = [line for line in 'updated_gold_standard.json']\n",
    "    subset_path_denot = os.path.abspath(\"gold_papers_tagged\") + \"/\"\n",
    "    denot_papers = [f for f in listdir(subset_path_denot) if isfile(join(subset_path_denot, f))]\n",
    "\n",
    "    evaluation_list = []\n",
    "\n",
    "    for j in range(len(gold_papers))\n",
    "        gold_papers_path = subset_path_gold + gold_papers[j]\n",
    "        dict_gold = json_to_dict(gold_papers_path)\n",
    "        denot_papers_path = subset_path_denot + denot_papers[j]\n",
    "        denotation = json_to_dict(denot_papers_path)\n",
    "\n",
    "        real_entities_retrived = find_real_entities_retrived(dict_gold, denotation)\n",
    "        real_entities = find_real_entities(dict_gold)\n",
    "        entities_retrived = find_entities_retrived(denot)\n",
    "\n",
    "        recall = Recall(real_entities, real_entities_retrived)\n",
    "        precision = Precision(real_entities_retrived,entities_retrived)\n",
    "\n",
    "        evaluation_list.append = {\"Article: denot_papers_path,\"recall\": recall, \"precision\": precision}\n",
    "    \n",
    "    print(evaluation_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-05-06"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-05-10\n",
    "Aim: fix so the evaluation code is divided in the different categories and get values of micro, macro, harmonic mean."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-05-18\n",
    "Aim: Make Pubannoation work, write on report, compare evaluators and implement code for generating runtime.\n",
    "\n",
    "The evaluators now give the same results which is great. Implementation of runtime code went good. We also improved the tagger so it takes care of hyphens. Uploaded a table with evaluation values on Github. Wrote on the report\n",
    "\n",
    "Next Step: Fix so Pubannotation works, write on report, fix presentation and comment the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
